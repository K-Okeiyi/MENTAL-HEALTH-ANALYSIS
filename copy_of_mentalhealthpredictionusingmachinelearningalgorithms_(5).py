# -*- coding: utf-8 -*-
"""Copy of MentalHealthPredictionUsingMachineLearningAlgorithms (5).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16R3HgutoUFqy0Bf16Kq58xC-76u4BEub

<a href="https://colab.research.google.com/github/cdodiya/Mental-Health-Prediction-using-Machine-Learning-Algorithms/blob/main/MentalHealthPredictionUsingMachineLearningAlgorithms.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

#Library and Data Loading
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from scipy import stats
from scipy.stats import randint

# prep
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.datasets import make_classification
from sklearn.preprocessing import binarize, LabelEncoder, MinMaxScaler

# models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier

# Validation libraries
from sklearn import metrics
from sklearn.metrics import accuracy_score, mean_squared_error, precision_recall_curve
from sklearn.model_selection import cross_val_score

#Neural Network
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import RandomizedSearchCV

#Bagging
from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier

#Naive bayes
from sklearn.naive_bayes import GaussianNB

!pip install mlxtend


#Stacking
from mlxtend.classifier import StackingClassifier



from google.colab import files
uploaded = files.upload()

uploaded.keys()

import pandas as pd

# Use the exact file name from the upload step
file_name = list(uploaded.keys())[0]  # This assumes only one file was uploaded
train_df = pd.read_csv(file_name)

# Print some basic information about the dataframe
print(train_df.shape)
print(train_df.describe())
print(train_df.info())

"""#Data Cleaning"""

#missing data
total = train_df.isnull().sum().sort_values(ascending=False)
percent = (train_df.isnull().sum()/train_df.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)
print(missing_data)

#dealing with missing data
train_df.drop(['comments'], axis= 1, inplace=True)
train_df.drop(['state'], axis= 1, inplace=True)
train_df.drop(['Timestamp'], axis= 1, inplace=True)

train_df.isnull().sum().max() #just checking that there's no missing data missing...
train_df.head(5)

"""Cleaning NaN"""

# Assign default values for each data type
defaultInt = 0
defaultString = 'NaN'
defaultFloat = 0.0

# Create lists by data tpe
intFeatures = ['Age']
stringFeatures = ['Gender', 'Country', 'self_employed', 'family_history', 'treatment', 'work_interfere',
                 'no_employees', 'remote_work', 'tech_company', 'anonymity', 'leave', 'mental_health_consequence',
                 'phys_health_consequence', 'coworkers', 'supervisor', 'mental_health_interview', 'phys_health_interview',
                 'mental_vs_physical', 'obs_consequence', 'benefits', 'care_options', 'wellness_program',
                 'seek_help']
floatFeatures = []

# Clean the NaN's
for feature in train_df:
    if feature in intFeatures:
        train_df[feature] = train_df[feature].fillna(defaultInt)
    elif feature in stringFeatures:
        train_df[feature] = train_df[feature].fillna(defaultString)
    elif feature in floatFeatures:
        train_df[feature] = train_df[feature].fillna(defaultFloat)
    else:
        print('Error: Feature %s not recognized.' % feature)
train_df.head()

#Clean 'Gender'
gender = train_df['Gender'].unique()
print(gender)

#Made gender groups
male_str = ["male", "m", "male-ish", "maile", "mal", "male (cis)", "make", "male ", "man","msle", "mail", "malr","cis man", "Cis Male", "cis male"]
trans_str = ["trans-female", "something kinda male?", "queer/she/they", "non-binary","nah", "all", "enby", "fluid", "genderqueer", "androgyne", "agender", "male leaning androgynous", "guy (-ish) ^_^", "trans woman", "neuter", "female (trans)", "queer", "ostensibly male, unsure what that really means"]
female_str = ["cis female", "f", "female", "woman",  "femake", "female ","cis-female/femme", "female (cis)", "femail"]

for (row, col) in train_df.iterrows():

    if str.lower(col.Gender) in male_str:
        train_df['Gender'].replace(to_replace=col.Gender, value='male', inplace=True)

    if str.lower(col.Gender) in female_str:
        train_df['Gender'].replace(to_replace=col.Gender, value='female', inplace=True)

    if str.lower(col.Gender) in trans_str:
        train_df['Gender'].replace(to_replace=col.Gender, value='trans', inplace=True)

#Get rid of bullshit
stk_list = ['A little about you', 'p']
train_df = train_df[~train_df['Gender'].isin(stk_list)]

print(train_df['Gender'].unique())

#complete missing age with mean
train_df['Age'].fillna(train_df['Age'].median(), inplace = True)

# Fill with media() values < 18 and > 120
s = pd.Series(train_df['Age'])
s[s<18] = train_df['Age'].median()
train_df['Age'] = s
s = pd.Series(train_df['Age'])
s[s>120] = train_df['Age'].median()
train_df['Age'] = s

#Ranges of Age
train_df['age_range'] = pd.cut(train_df['Age'], [0,20,30,65,100], labels=["0-20", "21-30", "31-65", "66-100"], include_lowest=True)

#There are only 0.014% of self employed so let's change NaN to NOT self_employed
#Replace "NaN" string from defaultString
train_df['self_employed'] = train_df['self_employed'].replace([defaultString], 'No')
print(train_df['self_employed'].unique())

#There are only 0.20% of self work_interfere so let's change NaN to "Don't know
#Replace "NaN" string from defaultString

train_df['work_interfere'] = train_df['work_interfere'].replace([defaultString], 'Don\'t know' )
print(train_df['work_interfere'].unique())

"""#Encoding Data"""

#Encoding data
labelDict = {}
for feature in train_df:
    le = preprocessing.LabelEncoder()
    le.fit(train_df[feature])
    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
    train_df[feature] = le.transform(train_df[feature])
    # Get labels
    labelKey = 'label_' + feature
    labelValue = [*le_name_mapping]
    labelDict[labelKey] =labelValue

for key, value in labelDict.items():
    print(key, value)

#Get rid of 'Country'
train_df = train_df.drop(['Country'], axis= 1)
train_df.head()

"""Testing there aren't any missing data"""

#missing data
total = train_df.isnull().sum().sort_values(ascending=False)
percent = (train_df.isnull().sum()/train_df.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)
print(missing_data)

"""Features Scaling: We're going to scale age, because it is extremely different from the other ones.

#Covariance Matrix. Variability comparison between categories of variables
"""

#correlation matrix
corrmat = train_df.corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True);
plt.show()

#treatment correlation matrix
k = 10 #number of variables for heatmap
cols = corrmat.nlargest(k, 'treatment')['treatment'].index
cm = np.corrcoef(train_df[cols].values.T)
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()

"""#Some charts to see data relationship

**Distribution** and density by Age
"""

# Distribution and density by Age
plt.figure(figsize=(12,8))
sns.distplot(train_df["Age"], bins=24)
plt.title("Distribution and density by Age")
plt.xlabel("Age")

"""Separate by treatment"""

import seaborn as sns
import matplotlib.pyplot as plt

# Create a FacetGrid with the height parameter
g = sns.FacetGrid(train_df, col='treatment', height=5)
g = g.map(sns.histplot, "Age")

# Show the plot
plt.show()

# Save the FacetGrid to a file
g.savefig('facet_grid.png')

"""How many people has been treated?"""

plt.figure(figsize=(12,8))
labels = labelDict['label_Gender']
g = sns.countplot(x="treatment", data=train_df)
g.set_xticklabels(labels)

plt.title('Total Distribution by treated or not')

"""Nested barplot to show probabilities for class and sex"""

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming labelDict is defined somewhere earlier in your code
# Example:
# labelDict = {
#     'label_age_range': ['<20', '20-29', '30-39', '40-49', '50-59', '60+'],
#     'label_Gender': ['Male', 'Female', 'Other']
# }

o = labelDict['label_age_range']

# Update the factorplot to catplot
g = sns.catplot(x="age_range", y="treatment", hue="Gender", data=train_df, kind="bar", ci=None, height=5, aspect=2, legend_out=True)
g.set_xticklabels(o)

plt.title('Probability of mental health condition')
plt.ylabel('Probability x 100')
plt.xlabel('Age')

# Replace legend labels
new_labels = labelDict['label_Gender']
for t, l in zip(g._legend.texts, new_labels):
    t.set_text(l)

# Positioning the legend
g.fig.subplots_adjust(top=0.9, right=0.8)

# Show the plot
plt.show()

# Save the plot to

"""Barplot to show probabilities for family history"""

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming labelDict is defined somewhere earlier in your code
# Example:
# labelDict = {
#     'label_family_history': ['No', 'Yes'],
#     'label_Gender': ['Male', 'Female', 'Other']
# }

o = labelDict['label_family_history']

# Update the factorplot to catplot
g = sns.catplot(x="family_history", y="treatment", hue="Gender", data=train_df, kind="bar", ci=None, height=5, aspect=2, legend_out=True)
g.set_xticklabels(o)

plt.title('Probability of mental health condition')
plt.ylabel('Probability x 100')
plt.xlabel('Family History')

# Replace legend labels
new_labels = labelDict['label_Gender']
for t, l in zip(g._legend.texts, new_labels):
    t.set_text(l)

# Positioning the legend
g.fig.subplots_adjust(top=0.9, right=0.8)

# Show the plot
plt.show()

# Save the plot to a file
g.savefig('probability_of_mental_health_condition_family_history.png')

"""Barplot to show probabilities for care options"""

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming labelDict is defined somewhere earlier in your code
# Example:
# labelDict = {
#     'label_care_options': ['Not available', 'Available'],
#     'label_Gender': ['Male', 'Female', 'Other']
# }

o = labelDict['label_care_options']

# Update the factorplot to catplot
g = sns.catplot(x="care_options", y="treatment", hue="Gender", data=train_df, kind="bar", ci=None, height=5, aspect=2, legend_out=True)
g.set_xticklabels(o)

plt.title('Probability of mental health condition')
plt.ylabel('Probability x 100')
plt.xlabel('Care options')

# Replace legend labels
new_labels = labelDict['label_Gender']
for t, l in zip(g._legend.texts, new_labels):
    t.set_text(l)

# Positioning the legend
g.fig.subplots_adjust(top=0.9, right=0.8)

# Show the plot
plt.show()

# Save the plot to a file
g.savefig('probability_of_mental_health_condition_care_options.png')

"""Barplot to show probabilities for benefits


"""

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming labelDict is defined somewhere earlier in your code
# Example:
# labelDict = {
#     'label_benefits': ['No', 'Yes', 'Don\'t know'],
#     'label_Gender': ['Male', 'Female', 'Other']
# }

o = labelDict['label_benefits']

# Update the factorplot to catplot
g = sns.catplot(x="care_options", y="treatment", hue="Gender", data=train_df, kind="bar", ci=None, height=5, aspect=2, legend_out=True)
g.set_xticklabels(o)

plt.title('Probability of mental health condition')
plt.ylabel('Probability x 100')
plt.xlabel('Benefits')

# Replace legend labels
new_labels = labelDict['label_Gender']
for t, l in zip(g._legend.texts, new_labels):
    t.set_text(l)

# Positioning the legend
g.fig.subplots_adjust(top=0.9, right=0.8)

# Show the plot
plt.show()

# Save the plot to a file
g.savefig('probability_of_mental_health_condition_benefits.png')

"""Barplot to show probabilities for work interfere


"""

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming labelDict is defined somewhere earlier in your code
# Example:
# labelDict = {
#     'label_work_interfere': ['Never', 'Rarely', 'Sometimes', 'Often'],
#     'label_Gender': ['Male', 'Female', 'Other']
# }

o = labelDict['label_work_interfere']

# Update the factorplot to catplot
g = sns.catplot(x="work_interfere", y="treatment", hue="Gender", data=train_df, kind="bar", ci=None, height=5, aspect=2, legend_out=True)
g.set_xticklabels(o)

plt.title('Probability of mental health condition')
plt.ylabel('Probability x 100')
plt.xlabel('Work interfere')

# Replace legend labels
new_labels = labelDict['label_Gender']
for t, l in zip(g._legend.texts, new_labels):
    t.set_text(l)

# Positioning the legend
g.fig.subplots_adjust(top=0.9, right=0.8)

# Show the plot
plt.show()

# Save the plot to a file
g.savefig('probability_of_mental_health_condition_work_interfere.png')

"""#Scaling and Fitting

Features Scaling We're going to scale age, because is extremely different from the othere ones.
"""

# Scaling Age
scaler = MinMaxScaler()
train_df['Age'] = scaler.fit_transform(train_df[['Age']])
train_df.head()

"""Spilitting Dataset"""

# define X and y
feature_cols = ['Age', 'Gender', 'family_history', 'benefits', 'care_options', 'anonymity', 'leave', 'work_interfere']
X = train_df[feature_cols]
y = train_df.treatment

# split X and y into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)

# Create dictionaries for final graph
# Use: methodDict['Stacking'] = accuracy_score
methodDict = {}
rmseDict = ()

# Build a forest and compute the feature importances
forest = ExtraTreesClassifier(n_estimators=250,
                              random_state=0)

forest.fit(X, y)
importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]

labels = []
for f in range(X.shape[1]):
    labels.append(feature_cols[f])

# Plot the feature importances of the forest
plt.figure(figsize=(12,8))
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices],
       color="r", yerr=std[indices], align="center")
plt.xticks(range(X.shape[1]), labels, rotation='vertical')
plt.xlim([-1, X.shape[1]])
plt.show()

"""#Tuning"""

def evalClassModel(model, y_test, y_pred_class, plot=False):
    #Classification accuracy: percentage of correct predictions
    # calculate accuracy
    print('Accuracy:', metrics.accuracy_score(y_test, y_pred_class))

    #Null accuracy: accuracy that could be achieved by always predicting the most frequent class
    # examine the class distribution of the testing set (using a Pandas Series method)
    print('Null accuracy:\n', y_test.value_counts())

    # calculate the percentage of ones
    print('Percentage of ones:', y_test.mean())

    # calculate the percentage of zeros
    print('Percentage of zeros:',1 - y_test.mean())

    #Comparing the true and predicted response values
    print('True:', y_test.values[0:25])
    print('Pred:', y_pred_class[0:25])

    #Confusion matrix
    # save confusion matrix and slice into four pieces
    confusion = metrics.confusion_matrix(y_test, y_pred_class)
    #[row, column]
    TP = confusion[1, 1]
    TN = confusion[0, 0]
    FP = confusion[0, 1]
    FN = confusion[1, 0]

    # visualize Confusion Matrix
    sns.heatmap(confusion,annot=True,fmt="d")
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

    #Metrics computed from a confusion matrix
    #Classification Accuracy: Overall, how often is the classifier correct?
    accuracy = metrics.accuracy_score(y_test, y_pred_class)
    print('Classification Accuracy:', accuracy)

    #Classification Error: Overall, how often is the classifier incorrect?
    print('Classification Error:', 1 - metrics.accuracy_score(y_test, y_pred_class))

    #False Positive Rate: When the actual value is negative, how often is the prediction incorrect?
    false_positive_rate = FP / float(TN + FP)
    print('False Positive Rate:', false_positive_rate)

    #Precision: When a positive value is predicted, how often is the prediction correct?
    print('Precision:', metrics.precision_score(y_test, y_pred_class))


    # IMPORTANT: first argument is true values, second argument is predicted probabilities
    print('AUC Score:', metrics.roc_auc_score(y_test, y_pred_class))

    # calculate cross-validated AUC
    print('Cross-validated AUC:', cross_val_score(model, X, y, cv=10, scoring='roc_auc').mean())

    ##########################################
    #Adjusting the classification threshold
    ##########################################
    # print the first 10 predicted responses
    print('First 10 predicted responses:\n', model.predict(X_test)[0:10])

    # print the first 10 predicted probabilities of class membership
    print('First 10 predicted probabilities of class members:\n', model.predict_proba(X_test)[0:10])

    # print the first 10 predicted probabilities for class 1
    model.predict_proba(X_test)[0:10, 1]

    # store the predicted probabilities for class 1
    y_pred_prob = model.predict_proba(X_test)[:, 1]

    if plot == True:
        # histogram of predicted probabilities
        plt.rcParams['font.size'] = 12
        plt.hist(y_pred_prob, bins=8)

        # x-axis limit from 0 to 1
        plt.xlim(0,1)
        plt.title('Histogram of predicted probabilities')
        plt.xlabel('Predicted probability of treatment')
        plt.ylabel('Frequency')


    # predict treatment if the predicted probability is greater than 0.3
    # it will return 1 for all values above 0.3 and 0 otherwise
    # results are 2D so we slice out the first column
    y_pred_prob = y_pred_prob.reshape(-1,1)
    y_pred_class = binarize(y_pred_prob, 0.3)[0]

    # print the first 10 predicted probabilities
    print('First 10 predicted probabilities:\n', y_pred_prob[0:10])

    ##########################################
    #ROC Curves and Area Under the Curve (AUC)
    ##########################################

    #AUC is the percentage of the ROC plot that is underneath the curve
    #Higher value = better classifier
    roc_auc = metrics.roc_auc_score(y_test, y_pred_prob)



    # IMPORTANT: first argument is true values, second argument is predicted probabilities
    # roc_curve returns 3 objects fpr, tpr, thresholds
    # fpr: false positive rate
    # tpr: true positive rate
    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)
    if plot == True:
        plt.figure()

        plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)
        plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.0])
        plt.rcParams['font.size'] = 12
        plt.title('ROC curve for treatment classifier')
        plt.xlabel('False Positive Rate (1 - Specificity)')
        plt.ylabel('True Positive Rate (Sensitivity)')
        plt.legend(loc="lower right")
        plt.show()

    # define a function that accepts a threshold and prints sensitivity and specificity
    def evaluate_threshold(threshold):
        #Sensitivity: When the actual value is positive, how often is the prediction correct?
        #Specificity: When the actual value is negative, how often is the prediction correct?print('Sensitivity for ' + str(threshold) + ' :', tpr[thresholds > threshold][-1])
        print('Specificity for ' + str(threshold) + ' :', 1 - fpr[thresholds > threshold][-1])

    # One way of setting threshold
    predict_mine = np.where(y_pred_prob > 0.50, 1, 0)
    confusion = metrics.confusion_matrix(y_test, predict_mine)
    print(confusion)



    return accuracy

"""Tuning with cross validation score"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier

def tuningCV(X, y, k_range=list(range(1, 31))):
    """
    This function performs cross-validation to find the optimal value of K for K-Nearest Neighbors (KNN).

    Parameters:
    X (array-like): Feature matrix.
    y (array-like): Target vector.
    k_range (list, optional): List of K values to try. Default is range(1, 31).

    Returns:
    None: This function plots the cross-validated accuracy for each K.
    """
    k_scores = []

    # Perform cross-validation for each value of k
    for k in k_range:
        knn = KNeighborsClassifier(n_neighbors=k)
        scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')
        k_scores.append(scores.mean())

    # Plotting the results
    plt.figure(figsize=(10, 6))
    plt.plot(k_range, k_scores, marker='o')
    plt.xlabel('Value of K for KNN')
    plt.ylabel('Cross-Validated Accuracy')
    plt.title('KNN Hyperparameter Tuning')
    plt.grid(True)
    plt.show()

    # Print the optimal value of K and the corresponding score
    optimal_k = k_range[np.argmax(k_scores)]
    optimal_score = max(k_scores)
    print(f'Optimal value of K: {optimal_k}')
    print(f'Cross-Validated Accuracy for optimal K: {optimal_score:.4f}')

# Example usage:
# tuningCV(X, y)

"""Tuning with GridSearchCV"""

def tuningGridSerach(knn):
    #More efficient parameter tuning using GridSearchCV
    k_range = list(range(1, 31))
    print(k_range)

    # create a parameter grid: map the parameter names to the values that should be searched
    param_grid = dict(n_neighbors=k_range)
    print(param_grid)

    # instantiate the grid
    grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')

    # fit the grid with data
    grid.fit(X, y)

    # view the complete results (list of named tuples)
    grid.grid_scores_

    # examine the first tuple
    print(grid.grid_scores_[0].parameters)
    print(grid.grid_scores_[0].cv_validation_scores)
    print(grid.grid_scores_[0].mean_validation_score)

    # create a list of the mean scores only
    grid_mean_scores = [result.mean_validation_score for result in grid.grid_scores_]
    print(grid_mean_scores)

    # plot the results
    plt.plot(k_range, grid_mean_scores)
    plt.xlabel('Value of K for KNN')
    plt.ylabel('Cross-Validated Accuracy')
    plt.show()

    # examine the best model
    print('GridSearch best score', grid.best_score_)
    print('GridSearch best params', grid.best_params_)
    print('GridSearch best estimator', grid.best_estimator_)

"""Tuning with RandomizedSearchCV"""

def tuningRandomizedSearchCV(model, param_dist):
    #Searching multiple parameters simultaneously
    # n_iter controls the number of searches
    rand = RandomizedSearchCV(model, param_dist, cv=10, scoring='accuracy', n_iter=10, random_state=5)
    rand.fit(X, y)
    rand.cv_results_

    # examine the best model
    print('Rand. Best Score: ', rand.best_score_)
    print('Rand. Best Params: ', rand.best_params_)

    # run RandomizedSearchCV 20 times (with n_iter=10) and record the best score
    best_scores = []
    for _ in range(20):
        rand = RandomizedSearchCV(model, param_dist, cv=10, scoring='accuracy', n_iter=10)
        rand.fit(X, y)
        best_scores.append(round(rand.best_score_, 3))
    print(best_scores)

"""Tuning with searching multiple parameters simultaneously"""

def tuningMultParam(knn):

    #Searching multiple parameters simultaneously
    # define the parameter values that should be searched
    k_range = list(range(1, 31))
    weight_options = ['uniform', 'distance']

    # create a parameter grid: map the parameter names to the values that should be searched
    param_grid = dict(n_neighbors=k_range, weights=weight_options)
    print(param_grid)

    # instantiate and fit the grid
    grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')
    grid.fit(X, y)

    # view the complete results
    print(grid.grid_scores_)

    # examine the best model
    print('Multiparam. Best Score: ', grid.best_score_)
    print('Multiparam. Best Params: ', grid.best_params_)

"""#Evaluating models

Logistic Regression
"""

def logisticRegression():
    # train a logistic regression model on the training set
    logreg = LogisticRegression()
    logreg.fit(X_train, y_train)

    # make class predictions for the testing set
    y_pred_class = logreg.predict(X_test)

    accuracy_score = evalClassModel(logreg, y_test, y_pred_class, True)

    #Data for final graph
    methodDict['Log. Regression'] = accuracy_score * 100

logisticRegression()

"""KNeighbors Classifier"""

def Knn():
    # Calculating the best parameters
    knn = KNeighborsClassifier(n_neighbors=5)

    # define the parameter values that should be searched
    k_range = list(range(1, 31))
    weight_options = ['uniform', 'distance']

    # specify "parameter distributions" rather than a "parameter grid"
    param_dist = dict(n_neighbors=k_range, weights=weight_options)
    tuningRandomizedSearchCV(knn, param_dist)

    # train a KNeighborsClassifier model on the training set
    knn = KNeighborsClassifier(n_neighbors=27, weights='uniform')
    knn.fit(X_train, y_train)

    # make class predictions for the testing set
    y_pred_class = knn.predict(X_test)

    accuracy_score = evalClassModel(knn, y_test, y_pred_class, True)

    #Data for final graph
    methodDict['K-Neighbors'] = accuracy_score * 100

Knn()

"""Decision Tree classifier"""

def treeClassifier():
    # Calculating the best parameters
    tree = DecisionTreeClassifier()
    featuresSize = feature_cols.__len__()
    param_dist = {"max_depth": [3, None],
              "max_features": randint(1, featuresSize),
              "min_samples_split": randint(2, 9),
              "min_samples_leaf": randint(1, 9),
              "criterion": ["gini", "entropy"]}
    tuningRandomizedSearchCV(tree, param_dist)

    # train a decision tree model on the training set
    tree = DecisionTreeClassifier(max_depth=3, min_samples_split=8, max_features=6, criterion='entropy', min_samples_leaf=7)
    tree.fit(X_train, y_train)

    # make class predictions for the testing set
    y_pred_class = tree.predict(X_test)

    accuracy_score = evalClassModel(tree, y_test, y_pred_class, True)

    #Data for final graph
    methodDict['Decision Tree Classifier'] = accuracy_score * 100

treeClassifier()

"""Random Forests"""

def randomForest():
    # Calculating the best parameters
    forest = RandomForestClassifier(n_estimators = 20)

    featuresSize = feature_cols.__len__()
    param_dist = {"max_depth": [3, None],
              "max_features": randint(1, featuresSize),
              "min_samples_split": randint(2, 9),
              "min_samples_leaf": randint(1, 9),
              "criterion": ["gini", "entropy"]}
    tuningRandomizedSearchCV(forest, param_dist)

    # Building and fitting my_forest
    forest = RandomForestClassifier(max_depth = None, min_samples_leaf=8, min_samples_split=2, n_estimators = 20, random_state = 1)
    my_forest = forest.fit(X_train, y_train)

    # make class predictions for the testing set
    y_pred_class = my_forest.predict(X_test)

    accuracy_score = evalClassModel(my_forest, y_test, y_pred_class, True)

    #Data for final graph
    methodDict['Random Forest'] = accuracy_score * 100

randomForest()

"""Bagging"""

def bagging():
    # Building and fitting
    bag = BaggingClassifier(DecisionTreeClassifier(), max_samples=1.0, max_features=1.0, bootstrap_features=False)
    bag.fit(X_train, y_train)

    # make class predictions for the testing set
    y_pred_class = bag.predict(X_test)

    accuracy_score = evalClassModel(bag, y_test, y_pred_class, True)

    #Data for final graph
    methodDict['Bagging'] = accuracy_score * 100

bagging()

"""Boosting"""

def boosting():
    # Building and fitting
    clf = DecisionTreeClassifier(criterion='entropy', max_depth=1)
    boost = AdaBoostClassifier(base_estimator=clf, n_estimators=500)
    boost.fit(X_train, y_train)

    # make class predictions for the testing set
    y_pred_class = boost.predict(X_test)

    accuracy_score = evalClassModel(boost, y_test, y_pred_class, True)

    #Data for final graph
    methodDict['Boosting'] = accuracy_score * 100

boosting()

"""Stacking"""

def stacking():
    # Building and fitting
    clf1 = KNeighborsClassifier(n_neighbors=1)
    clf2 = RandomForestClassifier(random_state=1)
    clf3 = GaussianNB()
    lr = LogisticRegression()
    stack = StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr)
    stack.fit(X_train, y_train)

    # make class predictions for the testing set
    y_pred_class = stack.predict(X_test)

    accuracy_score = evalClassModel(stack, y_test, y_pred_class, True)

    #Data for final graph
    methodDict['Stacking'] = accuracy_score * 100

stacking()

"""#Predicting with Neural Network

Create input function
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
import argparse

print(tf.__version__)

pip install tensorflow

import tensorflow as tf

# Define the feature columns and other necessary parts of your model here
feature_columns = [...]

# Define the DNNClassifier with the updated optimizer path
classifier = tf.estimator.DNNClassifier(
    feature_columns=feature_columns,
    hidden_units=[10, 10],  # Two hidden layers with 10 nodes each
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1)
)

import tensorflow as tf

# Define the feature columns
feature_columns = [...]

# Define the optimizer
optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.1)

# Build the DNNClassifier
classifier = tf.estimator.DNNClassifier(
    feature_columns=feature_columns,
    hidden_units=[10, 10],
    optimizer=optimizer
)

import tensorflow as tf
import pandas as pd
from sklearn.model_selection import train_test_split

# Example data setup
# Assuming `df` is your DataFrame with the last column as the label
# Replace this with your actual data loading code
data = {
    'feature1': [1, 2, 3, 4, 5],
    'feature2': [6, 7, 8, 9, 10],
    'label': [0, 1, 0, 1, 0]
}
df = pd.DataFrame(data)

# Split the DataFrame into features and labels
X = df.iloc[:, :-1]  # All columns except the last one
y = df.iloc[:, -1]   # The last column

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)

# Define feature columns
feature_columns = []
for key in X_train.keys():
    feature_columns.append(tf.feature_column.numeric_column(key=key))

# Input function for training
def train_input_fn(features, labels, batch_size):
    """An input function for training"""
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))
    # Shuffle, repeat, and batch the examples.
    return dataset.shuffle(1000).repeat().batch(batch_size)

# Input function for evaluation
def eval_input_fn(features, labels, batch_size):
    """An input function for evaluation or prediction"""
    features = dict(features)
    if labels is None:
        # No labels, use only features.
        inputs = features
    else:
        inputs = (features, labels)
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices(inputs)
    # Batch the examples
    assert batch_size is not None, "batch_size must not be None"
    dataset = dataset.batch(batch_size)
    # Return the dataset.
    return dataset

# Define the optimizer using tf.keras.optimizers.legacy.Adagrad
optimizer = tf.keras.optimizers.legacy.Adagrad(
    learning_rate=0.1
)

# Instantiate the DNNClassifier
classifier = tf.estimator.DNNClassifier(
    feature_columns=feature_columns,
    hidden_units=[10, 10],
    optimizer=optimizer
)

# Train the model
batch_size = 100
train_steps = 1000
classifier.train(
    input_fn=lambda: train_input_fn(X_train, y_train, batch_size),
    steps=train_steps
)

# Evaluate the model
eval_result = classifier.evaluate(
    input_fn=lambda: eval_input_fn(X_test, y_test, batch_size)
)

print(f"\nTest set accuracy: {eval_result['accuracy']:.3f}\n")

feature_columns = [
    tf.feature_column.numeric_column("feature1"),
    tf.feature_column.numeric_column("feature2")
]

optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)

import tensorflow as tf
import pandas as pd
from sklearn.model_selection import train_test_split

# Example data setup
data = {
    'feature1': [1, 2, 3, 4, 5],
    'feature2': [6, 7, 8, 9, 10],
    'label': [0, 1, 0, 1, 0]
}
df = pd.DataFrame(data)

# Split the DataFrame into features and labels
X = df.iloc[:, :-1].values  # All columns except the last one
y = df.iloc[:, -1].values   # The last column

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')  # Use 'sigmoid' for binary classification
])

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
batch_size = 100
num_epochs = 10  # Adjust as needed
model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs)

# Evaluate the model
eval_result = model.evaluate(X_test, y_test, batch_size=batch_size)
print(f"\nTest set accuracy: {eval_result[1]:.3f}\n")

"""Define the feature columns"""

# Define Tensorflow feature columns
age = tf.feature_column.numeric_column("Age")
gender = tf.feature_column.numeric_column("Gender")
family_history = tf.feature_column.numeric_column("family_history")
benefits = tf.feature_column.numeric_column("benefits")
care_options = tf.feature_column.numeric_column("care_options")
anonymity = tf.feature_column.numeric_column("anonymity")
leave = tf.feature_column.numeric_column("leave")
work_interfere = tf.feature_column.numeric_column("work_interfere")
feature_columns = [age, gender, family_history, benefits, care_options, anonymity, leave, work_interfere]

!pip install --upgrade tensorflow

import tensorflow as tf

optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)

import tensorflow as tf
from tensorflow.keras.layers import Dense

# Create feature columns (replace numeric_column with DenseFeatures)
feature_columns = [
    tf.feature_column.numeric_column("feature1"),
    tf.feature_column.numeric_column("feature2")
]

# Create the optimizer (use Adam optimizer)
optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)

# Create the DNN model using Keras
model = tf.keras.Sequential([
    Dense(10, activation='relu', input_shape=(2,)),
    Dense(10, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer=optimizer,
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Print a success message
print("TensorFlow code is fixed and ready to use!")

"""Instantiate an Estimator"""

import tensorflow as tf
from tensorflow.keras.layers import Dense

# Create feature columns (replace numeric_column with DenseFeatures)
feature_columns = [
    tf.feature_column.numeric_column("feature1"),
    tf.feature_column.numeric_column("feature2")
]

# Create the optimizer (use Adam optimizer)
optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)

# Create the DNN model using Keras
model = tf.keras.Sequential([
    Dense(10, activation='relu', input_shape=(2,)),
    Dense(10, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer=optimizer,
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Print a success message
print("TensorFlow code is fixed and ready to use!")

import tensorflow as tf
import pandas as pd
from sklearn.model_selection import train_test_split

# Example data setup
data = {
    'feature1': [1, 2, 3, 4, 5],
    'feature2': [6, 7, 8, 9, 10],
    'label': [0, 1, 0, 1, 0]
}
df = pd.DataFrame(data)

# Split the DataFrame into features and labels
X = df.iloc[:, :-1].values  # All columns except the last one
y = df.iloc[:, -1].values   # The last column

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')  # Use 'sigmoid' for binary classification
])

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
batch_size = 100
num_epochs = 10  # Adjust as needed
model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs)

# Evaluate the model
eval_result = model.evaluate(X_test, y_test, batch_size=batch_size)
print(f"\nTest set accuracy: {eval_result[1]:.3f}\n")

"""Train the model"""

# Train the model using the fit method
model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs)

import tensorflow as tf
import pandas as pd
from sklearn.model_selection import train_test_split

# Example data setup
data = {
    'feature1': [1, 2, 3, 4, 5],
    'feature2': [6, 7, 8, 9, 10],
    'label': [0, 1, 0, 1, 0]
}
df = pd.DataFrame(data)

# Split the DataFrame into features and labels
X = df.iloc[:, :-1].values  # All columns except the last one
y = df.iloc[:, -1].values   # The last column

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')  # Use 'sigmoid' for binary classification
])

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
batch_size = 100
num_epochs = 10  # Adjust as needed
model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs)

# Evaluate the model
eval_result = model.evaluate(X_test, y_test, batch_size=batch_size)
print(f"\nTest set accuracy: {eval_result[1]:.3f}\n")

"""Evaluate the model

Making predictions (inferring) from the trained model
"""

import tensorflow as tf
import pandas as pd
from sklearn.model_selection import train_test_split

# Example data setup
data = {
    'feature1': [1, 2, 3, 4, 5],
    'feature2': [6, 7, 8, 9, 10],
    'label': [0, 1, 0, 1, 0]
}
df = pd.DataFrame(data)

# Split the DataFrame into features and labels
X = df.iloc[:, :-1].values  # All columns except the last one
y = df.iloc[:, -1].values   # The last column

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')  # Use 'sigmoid' for binary classification
])

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
batch_size = 100
num_epochs = 10  # Adjust as needed
model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs)

# Evaluate the model
eval_result = model.evaluate(X_test, y_test, batch_size=batch_size)
print(f"\nTest set accuracy: {eval_result[1]:.3f}\n")

# Make predictions
predictions = model.predict(X_test, batch_size=batch_size)

# Process predictions if needed
predicted_classes = (predictions > 0.5).astype(int)  # Convert probabilities to class labels (0 or 1)
print(f"\nPredicted classes: {predicted_classes}\n")

import numpy as np

# Example data setup
X_train = np.array([[1, 2], [3, 4], [5, 6]])  # Replace with your actual data
y_train = np.array([0, 1, 0])  # Replace with your actual labels
predictions = np.array([[0.1], [0.9], [0.2]])  # Replace with your actual predictions

# Convert X_train to a DataFrame to get indices if necessary
import pandas as pd
X_train_df = pd.DataFrame(X_train)

# Iterate over the indices, true labels, and predictions
for idx, label, prediction in zip(X_train_df.index, y_train, predictions):
    predicted_class = (prediction > 0.5).astype(int)  # Assuming binary classification
    print(f"Index: {idx}, True Label: {label}, Predicted Class: {predicted_class[0]}")

# If you want to save results, convert them to a DataFrame
results = pd.DataFrame({
    'Index': X_train_df.index,
    'True Label': y_train,
    'Predicted Class': (predictions > 0.5).astype(int).flatten()
})

# Save results to CSV
results.to_csv('results.csv', index=False)

# Assuming you have already trained the model and obtained predictions
# Let's create a sample DataFrame for demonstration purposes
import pandas as pd

# Sample data (replace with your actual data)
X_train = pd.DataFrame({
    'feature1': [1, 2, 3, 4],
    'feature2': [0.5, 0.8, 1.2, 1.5]
})

# Sample predictions (replace with your actual predictions)
predictions = [
    {'class_ids': [1], 'probabilities': [0.2, 0.8]},  # Example prediction 1
    {'class_ids': [0], 'probabilities': [0.9, 0.1]},  # Example prediction 2
    {'class_ids': [1], 'probabilities': [0.3, 0.7]},  # Example prediction 3
    {'class_ids': [0], 'probabilities': [0.7, 0.3]}   # Example prediction 4
]

# Generate predictions and create a DataFrame
template = ('\nIndex: "{}", Prediction is "{}" ({:.1f}%), expected "{}')
col1, col2, col3 = [], [], []

for idx, input_data, p in zip(X_train.index, y_train, predictions):
    class_id = p['class_ids'][0]
    probability = p['probabilities'][class_id] * 100
    predicted_class = "Class 1" if class_id == 1 else "Class 0"

    col1.append(idx)
    col2.append(predicted_class)
    col3.append(input_data)

results_df = pd.DataFrame({'index': col1, 'prediction': col2, 'expected': col3})
results_df.head()

"""#Success method plot"""

def plotSuccess():
    s = pd.Series(methodDict)
    s = s.sort_values(ascending=False)
    plt.figure(figsize=(12,8))
    #Colors
    ax = s.plot(kind='bar')
    for p in ax.patches:
        ax.annotate(str(round(p.get_height(),2)), (p.get_x() * 1.005, p.get_height() * 1.005))
    plt.ylim([70.0, 90.0])
    plt.xlabel('Method')
    plt.ylabel('Percentage')
    plt.title('Success of methods')

    plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Sample data (replace with your actual data)
methodDict = {
    'Method A': 85.2,
    'Method B': 78.9,
    'Method C': 92.3,
    'Method D': 81.5
}

def plotSuccess():
    s = pd.Series(methodDict)
    s = s.sort_values(ascending=False)
    plt.figure(figsize=(12, 8))

    # Plot the bar chart
    ax = s.plot(kind='bar')

    # Annotate each bar with the percentage value
    for p in ax.patches:
        ax.annotate(f"{p.get_height():.2f}%", (p.get_x() * 1.005, p.get_height() * 1.005))

    plt.ylim([70.0, 90.0])
    plt.xlabel('Method')
    plt.ylabel('Percentage')
    plt.title('Success of methods')
    plt.show()

# Call the function to plot the success percentages
plotSuccess()

"""#Creating predictions on test set"""

# Generate predictions with the best method
from sklearn.ensemble import AdaBoostClassifier

clf = AdaBoostClassifier()
clf.fit(X, y)
dfTestPredictions = clf.predict(X_test)

# Write predictions to a CSV file
# Since we don't have any significant field, we save the index
results = pd.DataFrame({'Index': X_test.index, 'Treatment': dfTestPredictions})
# Save to file
# This file will be visible after publishing in the output section
results.to_csv('results.csv', index=False)
results.head()

from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import train_test_split
import pandas as pd

# Example data setup
data = {
    'feature1': [1, 2, 3, 4, 5],
    'feature2': [6, 7, 8, 9, 10],
    'label': [0, 1, 0, 1, 0]
}
df = pd.DataFrame(data)

# Split the DataFrame into features and labels
X = df.iloc[:, :-1]  # All columns except the last one
y = df.iloc[:, -1]   # The last column

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)

# Fit the AdaBoost model
clf = AdaBoostClassifier()
clf.fit(X_train, y_train)

# Generate predictions
dfTestPredictions = clf.predict(X_test)

# Create a DataFrame with the predictions
results = pd.DataFrame({
    'Index': X_test.index,
    'Treatment': dfTestPredictions
})

# Save to CSV file
results.to_csv('results.csv', index=False)

# Display the first few rows of the results
print(results.head())

"""#Submission"""

# We don't have any significative field so we save the index
results = pd.DataFrame({'Index': X_test.index, 'Treatment': dfTestPredictions})
results

